All the calculations and measurements where performed on Deneb. In order to observe the effects of distributed memory nodes (and not shared memory nodes), the computations where performed using 1 process per node. The CPU which are used are either: 2.6GHz with 64GB of DDR3 RAM, or 2.5 GHz with 64GB of DDR4 RAM. The assignment to each node is unknown, hence the computations can be done on either of the two different CPUs, or a combination of both CPU types for multiple nodes. The speedups are all calculated based on the time required to compute one time iteration for the N-body problem, hence it is the time taken to finish the first synchronization step shown in Figure \ref{fig:timing}.\\
\subsection{Brute-force performance}
As the brute-force approach is very expensive in computations, the speeds are very good. In Figure \ref{fig:speedup:bf} the theoretical speedup is shown along with the measured ones. Up to 64 nodes, the theoretical speedup is still linear, and the measured speedup follows this path closely. When the number of nodes gets larger than 20, there is slight jump downwards in the speedup values for the simulation with $10^6$ bodies. This comes from the different computation times, observable in Table \ref{tab:bf:10e6}. Since two different CPUs can be used, with different clock rates and RAM access speeds, the same computation will take slightly different times. This is not observable for the simulation with fewer bodies, but becomes significant for large numbers of bodies. In Figure \ref{fig:bf:computeTimes} the parallel compute times normalized by the serial compute times are shown. Two groups of compute times stand out, most certainly based on the two different CPUs.\\

\begin{figure}
\centering
\input{../figures/speedup_bf.tex}
\caption{Speedup chart for the brute-force approach.}
\label{fig:speedup:bf}
\end{figure}

\begin{table}
  \begin{adjustwidth}{-1.5cm}{-1.5cm}   
    \centering
    \begin{tabular}{l|cccccccccc}
      \# nodes & 1 & 2 & 4 & 8 & 12 & 16 & 20 & 24 & 28 & 32\\
      \hline init. Bcast [s] & 0.008 & 0.022 & 0.028 & 0.027 & 0.0367 & 0.028 & 0.034 & 0.034 & 0.03 & 0.059\\
      comput. [s] & 44875 & 22945 & 11784 & 5626 & 2982 & 2826 & 1795 & 1490 & 1619 & 1117\\
      & - & - & - & 6026 & 3539.5 & - & 2359 & 1960 & - & 1480\\
      sync. [s] & 0.0032 & 0.0041 & 0.0049 & 0.0056 & 0.0072 & 0.0062 & 0.0083 & 0.0085 & 0.0076& 0.0079\\
    \end{tabular}
    \caption{Execution times for the brute-force approach, with $10^6$ bodies. The second line of computation times is for the simulations where two distinct times are observed, see Figure \ref{fig:bf:computeTimes}.}
    \label{tab:bf:10e6}
  \end{adjustwidth}
\end{table}

\begin{figure}
\centering
\input{../figures/computeTimesNormalizedBF10e6.tex}
\caption{Compute times for the brute-force approach using $10^6$ bodies, normalized by the number of bodies and number of nodes. There are two cases, either the compute times all have approximately the same value, or there are two groups of points. The different CPUs are suspected to be the origin for this differentiation.}
\label{fig:bf:computeTimes}
\end{figure}

%\begin{table}
%\centering
%\begin{tabular}{l|cccccccccc}
%\# nodes & 1 & 2 & 4 & 8 & 12 & 16 & 20 & 24 & 28 & 32\\
%\hline initial broadcast [s] & $6.2\cdot 10^{-4}$ & $2.7\cdot 10^{-3}$ & $0.0037\cdot 10^{-3}$ & $4.4\cdot 10^{-3}$ & $3.8\cdot 10^{-3}$ & $1.2\cdot 10^{-2}$ & $3.8\cdot 10^{-3}$ & $4.4\cdot 10^{-3}$ & $3.95\cdot 10^{-3}$ & $4.4\cdot 10^{-3}$\\
%computations [s] & 453.94 & 225.15 & 114.6 & 57.93 & 38.46 & 28.37 & 22.67 & 18.95 & 16.5 & 15.31\\
%synchronization [s] & $3.13\cdot 10^{-4}$ & $6.1\cdot 10^{-4}$ & $0.00078\cdot 10^{-4}$ & $8.1\cdot 10^{-4}$ & $1.1\cdot 10^{-3}$ & $1.5\cdot 10^{-3}$ & $1.1\cdot 10^{-3}$ & $1.2\cdot 10^{-3}$ & $1.1\cdot 10^{-3}$ & $1.0\cdot 10^{-3}$\\
%\end{tabular}
%\caption{$10^5$ bodies}
%\end{table}
\subsection{Barnes-Hut performance}
The speedup measurements for the Barnes-Hut algorithm follow the theoretical speedup, as long as the problem size is larger enough for the number of nodes. For $10^6$ bodies, 32 nodes is still interesting, but the measured speedup starts to leave the theoretical estimation. There is an odd bump in the speedup measurements for 12, 16 and 20 nodes. This could be due to the different CPUs as mentioned previously, or the load balancing is not sufficiently reliable. The second aspect is discussed in the following section. For the simulations with $10^5$, from 16 nodes it is clear that the problem is too small for the number of nodes which is used. The speedup does not go pas the value of 10, and would certainly even decrease for even larger numbers of nodes.\\
The issue with the Barnes-Hut algorithm in parallel is the load balancing of each node. In Tables \ref{tab:qt:10e5} and \ref{tab:qt:10e6}, the standard deviations of the computation times are reported next to the mean values. For both problem sizes, the variations stabilize for the larger number of nodes. This means that as the computation time gets reduced, the idle time of the nodes waiting for the slowest node to finish will become significant.
\begin{figure}
\centering
\input{../figures/speedup_qt.tex}
\caption{Speedup chart for the quad-tree approach.}
\end{figure}

\begin{table}
  \begin{adjustwidth}{-1.5cm}{-1.5cm}       
    \centering
    \begin{tabular}{l|cccccccccc}
      \# nodes & 1 & 2 & 4 & 8 & 12 & 16 & 20 & 24 & 28 & 32\\
      \hline init. Bcast [s] & $1.9\cdot10^{-6}$ & 0.0036 & 0.0048 & 0.0031 & 0.0039 & 0.0036 & 0.0037 & 0.0059 & 0.0066 & 0.0077\\
      qt build [s] & 0.202 & 0.201 & 0.202 & 0.205 & 0.204 & 0.203 &0.207 & 0.201 & 0.200 & 0.261\\
      comput. [s] & 8.24 & $4.12$ & 2.06 & 1.03 &  0.68  & 0.517 &0.420 & 0.315 & 0.262 & 0.310\\
      & - & $\pm 0.0055$ & $\pm 0.008$ & $\pm0.07$ & $\pm 0.04$ & $\pm 0.04$ & $\pm0.04$ & $\pm0.03$ & $\pm0.03$ & $\pm 0.04$ \\
      sync. [s] & $1.4\cdot 10^{-3}$ & 0.0017 & 0.0025 & 0.0014 & 0.0026 &  0.0036& 0.0034 & 0.0039 & 0.0041 & 0.0036\\
    \end{tabular}
    \caption{For $10^5$ bodies, measurements of each operation on Deneb using the Barnes-Hut algorithm.}
    \label{tab:qt:10e5}
  \end{adjustwidth}
\end{table}

\begin{table}
  \begin{adjustwidth}{-1.5cm}{-1.5cm}   
\centering
\begin{tabular}{l|cccccccccc}
\# nodes & 1 & 2 & 4 & 8 & 12 & 16 & 20 & 24 & 28 & 32\\
\hline init. Bcast [s] & $2\cdot10^{-6}$ & 0.029 & 0.041 & 0.041 & 0.047 & 0.043 & 0.053 & 0.048 & 0.031 & 0.044\\
qt build. [s] & 2.37 & 2.32 & 3.05 & 3.03 & 3.04 & 3.03 & 3.03 & 2.33 & 2.72 & 2.68\\
comput. [s] & 97.8 & 48.7 & 28.64 & 11.77 & 9.89 & 7.45 & 6.012 & 3.83 & 3.9 & 3.43 \\
& - & $\pm 0.56$ & $\pm 0.71$ & $\pm 0.82$ & $\pm 0.72$ & $\pm 1.05$ & $\pm 0.43$ & $\pm 0.25$ & $\pm 0.48$ & $\pm 0.50$  \\
sync. [s] & 0.015 & 0.015 & 0.019 & 0.025 & 0.026 & 0.026 & 0.031 & 0.029 & 0.025 &0.024\\
\end{tabular}
\caption{For $10^6$ bodies, measurements of each operation on Deneb using the Barnes-Hut algorithm.}
\label{tab:qt:10e6}
  \end{adjustwidth}
\end{table}
\newpage
\subsection{Load balancing performance}
In Figure \ref{fig:computeTimes:allnodes}, the computation time for each time step, for each node is shown. The node assignment is performed according to the algorithm presented previously. The setup for this simulation is two masses of $1500$ bodies each, which are destined to collide at some point. For the first phase, while the two masses are compact and the bodies are attracted to the respective centers, the computation increases on all nodes since the bodies are getting closer and close together. Once the bodies start to move away from the centers, some will leave the gravitational field of the local system due to numerical integration errors over time, whilst the others will start gravitating around the heavy body located in the respective centers. Once the bodies are spread out in space randomly, the load balancing becomes critical.\\
The second node ($node_1$), has more a longer computation time than the other nodes until iteration $\#$ 3800. This is the case since it has been assigned approximately 750 nodes which are much closer together than the bodies assigned to the other nodes. Indeed, if a group of 100 bodies is close together, than it will take more computations to calculate their interactions than 100 bodies which are far apart. The same applies to one group of 100 bodies, or 10 groups of 10 bodies. Therefore the actual node assignment is acceptable, but could be improved by taking into account the density of bodies as well.\\
\begin{figure}[H]
\centering
\input{../figures/timing_two_nodes.tex}
\caption{Compute time for the quad tree approach, using four nodes and 3000 bodies.}
\label{fig:computeTimes:allnodes}
\end{figure}
\newpage
When considering nodes 0 and 1 (black and blue lines), between iterations $\#4500$ and $\#6000$, the effect of assign a group of bodies can be observed, see Figure \ref{fig:computeTimes2Nodes:zoom}. As a group of bodies which are compact are assigned either nodes 0 or 1, the compute time jumps. This happens since this groups of bodies is time consuming to calculate, and will create an in balance in the nodes if poorly assigned. The limits of assigning the bodies only by space position, and not density, stand out here. To solve this issue, one could implement a body assignment strategy which takes into account the number of bodies which must be assigned and also the density of bodies. Each node could be given a capacity expressed in $nbBodies \cdot density$ to solve this issue.
\begin{figure}[h]
\centering
\input{../figures/timing_two_masses_zoom.tex}
\caption{Compute time for the quad tree approach, using four nodes and 3000 bodies.}
\label{fig:computeTimes2Nodes:zoom}
\end{figure}
